{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Videos and Exercises for Session 10: Introduction to Modelling and Machine Learning\n",
    "\n",
    "In this notebook, you will mainly be working with the Perceptron model. First, you will be introduced to theoretical aspects and applications for the model $-$ then you will be asked to produce a version of it yourself in a series of smaller steps. \n",
    "\n",
    "This part might take many of you some time to code up, but it is worth the effort. Not only should it help you increase your understanding of basic machine learning machinery. It should also be helpful for assignment 2 :) \n",
    "\n",
    "Part 2 mainly consists of a series of bonus exercises where you will be working with Adaline $-$ an extension of the perceptron. If you do not have time to solve the exercises, make sure to watch the videos! \n",
    "\n",
    "The structure is as follows:\n",
    "1. The Perceptron Model\n",
    "    - Implementing and Using the Model in Python\n",
    "    - Validation of a Model\n",
    "2. Beyond the Perceptron Model\n",
    "    - Logistic Regression\n",
    "    - Adaline\n",
    "    \n",
    "**NOTE:** I may be speaking quite slowly in some of the videos. A good advice is to turn up the speed to x1.25 or x1.5 if you want to get through without spending too much time:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Perceptron Model\n",
    "\n",
    "The first supervised learning model that we will introduce is an old model. We will learn about it because it is simple enough to grasp how it works, and we will use to build the intuition for more advanced models. The video below introduces the model theoretically with mathematics. \n",
    "\n",
    "Parts of the talk will use matrices to make computations. Thus, you may want to re-familiarize yourself with [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://www.youtube.com/embed/mN1Y_rkfZNY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x20b53ae0ca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('mN1Y_rkfZNY', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Using the Model in Python\n",
    "\n",
    "We now implement show you how the Perceptron model can be implemented in Python. First, we show you how you can initialize parameters. Then we show you how you can compute errors and update weights in a _vectorized_ way. As you may remember from the previous video, the Perceptron weights are typically updated one observation at a time - and this is also how you will implement it in a simple version below :). However, most alternative algorithms will use vectorized errors (for instance Adaline further down) on either the full set of data or a subset of data, which is why we show this in the video.\n",
    "\n",
    "The video also shows how we can use others' code - in this case, we use Raschka's implementation of the Perceptron from PML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://www.youtube.com/embed/8ZbXSuoGieg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x20b53ac3130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('8ZbXSuoGieg', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of a Model\n",
    "\n",
    "We want to have a credible measure of model performance. In this video, we talk about a simple approach to getting such a measure for cross-section/static data (i.e. not time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhsaGRoeHRsfIy0mIiIiIyUlJycnLyc1MC0nLy01PVBCNThLOS0vRWFFS1NWW11bNUFlbWRYbFBZW1cBERISGRYZMBsbMFg9Nz9XXVdXV1dgV1dXV1dXV1ddV1dXV1dXV1dXV1ddV1dXV1dXV11XV1dXV1dXXVdXV1dXV//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEBAQEAAwEAAAAAAAAAAAAAAQIGAwQFB//EAD8QAQABAgAMBAQDBwMEAwAAAAABAhEDBBIWIVFSYZKh0dIxQXGRgaKx8BUywQUTImKCwuEGM2NCQ3LxFCNT/8QAGgEBAQADAQEAAAAAAAAAAAAAAAECAwQFBv/EAC0RAQACAgECBQMDBAMAAAAAAAABAhETUQMUBBIhMUEigbEV0eEFMzRSYXGh/9oADAMBAAIRAxEAPwD8/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHQ5nYzt4Hir7TM7GdvA8Vfa2ar8Num/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7TM7GdvA8Vfaar8Gm/Dnh0OZ2M7eB4q+0zOxnbwPFX2mq/Bpvw54dDmdjO3geKvtMzsZ28DxV9pqvwab8OeHQ5nYzt4Hir7XrY5/pvDYHJyqsHOVe1pq8rbt7G1ZpGbezC9LUr5rRiHxx7/4ThNdHvPQ/CcJro956NO2nLRu6fL0B7/4ThNdHvPQ/CcJro956G2nJu6fL0B7/wCE4TXR7z0PwnCa6Peehtpybuny9Ae/+E4TXR7z0PwnCa6Peehtpybuny9Ae/8AhOE10e89D8Jwmuj3nobacm7p8v0XD4TJiNczERqvLOCwlWVNFcRlWvova17PJXRFUWnlomJ1wzgsDFPnMzrqm821PVmLeb/h7cxbzens8gDY2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4/+oP+1/V/a+w+P+34/wBr+r+1yeN/s2+35cX9Q/x7fb8w+MLZYpeA+XZFstgZGYri9lqqsuFxKiUTdpBGZnTO769GmKvCqL+Mxb2/xzZUjMunwlK26n1cS7QB9Q+rAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHx/wDUEf7X9X6PsPkft+f9v+r9HJ43+zb7flxf1D/Ht9vzD48Gn3InRYip4D5cCKi4rw0YOmrCRFVUU0+czofSrxTAURlVVREe75+DwMV1Tpj0teX0sSxenCYCiKtFpm0+caXZ0cTGMZe/4Hp56eJpGfeJl6uFxammMvB1ZVM63ifTw+LU04OqIrmqudMzNtNvLQ+ZLT1q+WzzvH9HV1I9MZePC4amnxKK4r0xL1scwc5V4i8NYlg5iZmYsx8sRXLXHTrXpbIn1d2A+mfVAM4Sq1MzqiZ5ITOGh6mCxqbVTVaaYiNNNM02mdMxOVPlFpvvh5ZxqmLzptHnabflyvo1x1azGctVevSYzl5h60Y3aa8qJjJnwtpimKaZmqeJascpi+iq0RVMzbRaibVT8F205N1OXsD18aw1VFVMRMReKp001V+FtFqZ3r/8qI0VxNNWTlTHj5ReI85tc2VzMSbq5mJ+HnHrTjM3mMmYtVERfxn/AOuavfQtGM3imZibzEWiNM6YvKbacm6nL2B62ExymIqmJ8InTpt+WKvpN25xmm8xp84jR4zFeRMR8ZiPiu2nK7qcvMM4OvKi9pjTMadcTaecNM4nPrDZExMZgAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHx/9Qf9r+r+19h6mPYnOFmiYrycm/le97b41OfxNJv0prX3/lzeM6dup0ZrX3/lzI+7V+yKp8cL8kdVj9lVWt+9jR/xx1+/d5HZdXj8fu8L9P63H4/d8Efcp/Y9URERhtGrI3W17oX8Jq//AG87/kjTOvxOy6vH4/c/T+tx+P3c5V/DVeHvfs6vB0TE/uq5q12vEPoYX9g5fjhfk/y1gf2BTRN/3lU7vCG2nhetE5w7vDeH69JjMe3/AE9PGMJkzNUefR6L72E/Y0VR/uTH9N/1eL8C/wCX5P8ALHqeF61p9mvxnhvEdXqZiMx9nxlfY/Av+X5P8n4F/wAvyf5a+z63+v4cf6f4j/X/ANh9gB9A+oEqpvExriypXVaJnVF0kn29XiqxaJmJvMTFp0W8omn6Tb2P/jU2yZvb1/kyPo1Tho9vHx39J9j99F7adXx0dYasdNpivSnh45xSJveqqZqvlTeNMTERMTaPC1MeDVWK0zExptMVx4+VdWVVzajC+GidN+UxH6n76NO7x1R6yeXpwnl6UN1UxNUVecRMe8xf6Q8U4tEzM3nT5aLXmIiZ5Q3OE02tPhE+/hCThdNoiZ0RPwmJt9GU+SfdlaKT7k4GJqypv4xVbyvFM039p5QzGLRaLVVRMWtOi9oi2rV93aqw0ZNUxpyb+N4jRe+n4SVYWIvfyi82+/X2SYok16bx1YnRMTFpiJ1T/JkfSDD4rFVMxHj/ABePh/FhIrqjwnVqn4vJOGi8x5x1mPrFmpr0zu+ujRzhJp05iUmnSmJYxeiaabTbR4Wt4fCIjlDysRhY0b5tHhp+7p+9jTuqyfWcmKvpPJnExEYbKzWsYiXkEpm8eHnMe02VkzicgCgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATAAmTF72i/oZMao9oUTBiEyY1Ro3QRTEeERHwhQxCYhJpjVHtBaNUeyhhcQmTGnRGnx0eJNMT4xE/CFDBiEmiNUe0a7/UyY1R7QoYgxDORGzHtH35QZEao07oaDEJiEtGqPZQFAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABmumqZptVaInToveLeG5pmuu00xaZvNtEXto8Z3JOMeqWxj1XTlRq+9H3q3vHg4r0XnR6Rfw859Wqq/4opjz8d3/vSkYaLxFpvOrT52YTMZ92uZrn1liKa489O+06/8NzFWjx8N2vxt6JOG0TMxNovymxVhrW0aNPLR+qfTyn08k5ejTp030Rbxi3wIirTN9Not4RpvP6TBOHi8Rabz/jrBGMRMX9OcRP8AdB9PJ9OfciK9fnu8MrT8bX8iiK9GVOq9reOi/wDdyajCbp/6fm8Gf3szNqd95ndNifLHyT5Y+UmK4pjTptqibTaPDRru1XlabfDw1T+tmZxiMmaoidFM1adHhF1nDRF91+UTP9sma8ma8rXFdptMf9Xjbfk/oTl30TFrzv0eUzzJwtraJ0xexGGifDT4ek3mIiYnVpiV9OV+nlqi/nf421bmninDao1+8W0cz9/F7Wnxt5a4jXrlYtEMovWPTLyjx04aJimbT/F4aGacPFqZmJvMR4RPnFzzwbK8vMAzZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABcYj88+kfWQmW7l1gQZuXUBLwXg06gMl4LwKCXgvCiiXgmygJFo8C8KIJo3F43KKJERBeFEEuXUUS5dQEuXUBLl1AS5dQEuXUBLl1AS5dQEuXUBLl1AS5dQEuXUBLl1AS5dQEuXUBLl1AS5dQEuoAAAAAMR+efSPrLbEfnn0j6yJPw1NUa0/eRrhaqInxhIojUMvRYqifCVSKYjw+/u5fd9BJUZmZ8o+iTlJlPM2M1TN9HhzKZnzj2MmWhmrK8o97dSZny52MnmaGLzaNfwImbTePQyeZsYjK58msrd9DJlRiL/D4H8Rk8zYxGVfcszOr6GSLNDMTPnHsTM+X6dTJloZmZtGv4Gn7sZPM0MXnT/gjK+7GTzNjE5XkfxJlPM2Mxfz/AE6pGVov8VyuWxicry++bUeGkyROVAVQAAAAAAAAAAAAAAAAAAAAgAAAAAGI/PPpH1ltiPzz6R9ZEn4bAFEtpVnJ0/ljl7pKS1aS0sZEX/LHyrkRsx8oNWneZMs5EbMfKTRGzHyoi77T7ExP3djJ3R8vVZpjZj5RG7To/wAlpYyI0fwx8q5EbMfKqtWlJidX1TIjZj5UmjdHyoS1adX1IiWcmNmORFEbMfKEN2ktLORGzHypkRs/QJbtO9IjdPsmRGzHypkfyx8oS1MTqn2IjdMMzRGzHIin+WPlD5btKRRuSaI2Y+VIo/lj2jqEtZO6UmNHhPsmR/LHymRGzHyojeTKRHr7JFMbMfKkUfyx8vVVW2uJn4NMZEao5dWojdb2WCPdQFZAAAAAAAAAAAAAAAAAAAAAAAAAADEfnn0j6y2xH559I+siT8NgCjPn5+Hjo9mmZmI8Y+nVJSVtvktvP6Z5dUv/ACzy6oi23yW3nwnl1T4Ty6gW9eRMafOV+E8uqTO6eXUFtvktvT+meXVfhPLqBbekx6l/5Z5dVvunl1CUiPVYj1L7p5dUj0nl1Bbb0mPv7hdGqeXVL7p5dQW2+UiPXksTunl1TRq+nUCYWI9U+E8uq/CeXUCYSPjyJ/8AGeXUvunl1AiPUmPUyt08upeNU8uomFiN8pEb5X+meXVL7p5dRS3ryWC+6eXVYWFgAVQAAAAAAAAAAAAAAAAAAAAAAAAABiPzz6R9ZbYj88+kfWRJ+GwBRiqYv/6bZqnfMe3Vjb2Y39i8a+cdFvGufeOheN/Lqt908uoM3i/jzjoXjXzjot/Xl1L+vLqqs3jx03+HRZmNfOOi33z7U9SqdPnHt1RikzGjTzjoXjX9Oi38PH2jqX9eXUVLxr5x0JmNc8uixO6eXUv/AOXt/kklImNc8uhExrnl0W/ry6kT6+0dSCC8a5946JNtf06NX3Ty6p78uoSl41846F41846NRPry6plb59o6gkzGufeOhExr+nRZn/y9o6kT6z8I6h8peNc+8dCLa/p0W/ry6nF7R1CUvGueXRLxrn3jx9mr+vtHUvvnl1REiY1z7x0ImNc8ui39eXUv68uqqzNtf06NwmVvn2jqqwQAKyAAAAAAAAAAAAAAAAAAAAAAAAAAGI/PPpH1ltiPzz6R9ZEn4bAFGK4n7t0bZmtJSy23zy6Ft88uhlQZQehk755dC2+eXRIqWKovYPQyd8/L0Sad88uixN1MGEyd88uhk755dFDBhLb55dEmnfPLo0GDCRTvnl0IjfPLooYMJbfPLoZO+eXRQwYSI3zy6GT68uihgwlt88uiRTvmPbo0GDCZO+eXRIid/wAvRoMGGZj1+XouT68uiiYTypEb55dCKd88uii4XDOTvnl0aAXAAoAAAAAAAAAAAAAAAAAAAAAAAAAAMR+afSPrLYEwAAAgKERo/Rm+6fu3XkhloS57qKMV12tombzbRbVM/o1M28pSJiUiYn0UYy90rFV/KVZNCXL6JnV5adKJlRLl/VRQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEUAvFv1ZiJ18vT/AD7rYsiYL71Syis1UTMxN7Wm/humP1WyhhMQlt4oLhm03jSttPiophCFEABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJqiPGU/eU6/v7lZpifGIlMiNUC+hTXE+ErcimI8lsIly5ZJ8J9EGpGavDw1+K+QF1TyIUVCkhDKgKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkzaLpg8JFXhfR4xMWmPgmY9kzGcNAKoAACXBUnzMpJ+qBPh5yvkk75+7SvqB5ffsUkl7AU+CUrGiGb6PHl96xGxmZjX96T4/ekyZaEsRAqgKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALEAgCTMQkzEe4zg6MmLOJzwxnYwXDV3GeGM7GC4au5o30c/c9P3dwOHzwxnYwXDV3GeGM7GC4au5d9F7qjuBw+eGM7GC4au4zwxnYwXDV3G+h3VHcDh88MZ2MFw1dxnhjOxguGruN9DuaO4HD54YzsYLhq7jPDGdjBcNXcb6Hc0dwOHzwxnYwXDV3GeGM7GC4au430O5o7gcPnhjOxguGruM8MZ2MFw1dxvodzR3A4fPDGdjBcNXcZ4YzsYLhq7jfQ7mjuBw+eGM7GC4au4zwxnYwXDV3G+h3NHcDh88MZ2MFw1dxnhjOxguGruN9DuqO4HD54YzsYLhq7jPDGdjBcNXcb6HdUdwOHzwxnYwXDV3GeGM7GC4au430O6o7gcPnhjOxguGruM8MZ2MFw1dxvod1R3A4fPDGdjBcNXcZ4YzsYLhq7jfQ7qjuBw+eGM7GC4au4zwxnYwXDV3G+h3VHcDh88MZ2MFw1dxnhjOxguGruN9DuqO4HD54YzsYLhq7jPDGdjBcNXcb6HdUdwOHzwxnYwXDV3GeGM7GC4au430O6o7gcPnhjOxguGruM8MZ2MFw1dxvod1R3A4fPDGdjBcNXcZ4YzsYLhq7jfQ7qjuBw+eGM7GC4au4zwxnYwXDV3G+h3VHcDh88MZ2MFw1dxnhjOxguGruN9DuqO4HD54YzsYLhq7jPDGdjBcNXcb6HdUdwOIj/WOM7GB4au4zxxnYwPDX3G+h3NHbjiM8cZ2MDw19xnjjOxgeGvuN9DuaO3HEZ44zsYHhr7jPHGdjA8Nfcb6Hc0duOIzxxnYwPDX3GeOM7GB4a+430O5o7ccRnjjOxgeGvuM8cZ2MDw19xvodzR244jPHGdjA8NfcZ44zsYHhr7jfQ7npu5ijRe8W+9DLiM8cZ2MDw19xnjjOxgeGruTdT5TuOn8ueAcTzgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH//2Q==\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://www.youtube.com/embed/-9L0dUg0xvY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x20b53a6f100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('-9L0dUg0xvY', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overblik over exercise 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10.1.1: Import numpy, pandas, and seaborn.\n",
    "- 10.1.2: Load the iris data, filter it, and create train-test datasets.\n",
    "- 10.1.3: Write a function to initialize a set of weights.\n",
    "- 10.1.4: Write functions for net input and prediction, and optionally, a function to calculate accuracy.\n",
    "- 10.1.5: Write a function that updates the perceptron weights using the training data.\n",
    "- 10.1.6: Write a function that trains the perceptron over multiple epochs and records the errors.\n",
    "- 10.1.7 (BONUS): Use the updated weights to predict and calculate the accuracy on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex. 10.1.1:** The mathematics and biological reasoning which justifies the perceptron model is discussed in PML.\n",
    ">\n",
    "> Begin by importing `numpy`, `pandas` and `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34f93fe2b73cb28e99c5efd31a15b13f",
     "grade": false,
     "grade_id": "cell-83b57b9149cb54a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.2:** Use the following code snippet to load the iris data. The code will create two new variablex **X** and **y**, each of which are numpy arrays. Split the data as follows:\n",
    "1. The first dataset should contain the first 70 rows; we call this sample our *training dataset*, or simply *train data* (`Xtrain` and `ytrain`). We use the training data to estimate the data. \n",
    "2. We use the remaining rows as data for testing our model, thus we call it *test data* (`Xtest` and `ytest`). \n",
    ">\n",
    ">```python \n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac=1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "sns.pairplot(iris, hue=\"species\", palette=\"husl\", diag_kws = {'shade': False})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5f974f5113601ea0f4439f43bbc91f3",
     "grade": false,
     "grade_id": "cell-44861170623379b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Loading the iris dataset and filtering it\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac=1, random_state = 3)\n",
    "\n",
    "# Creating numpy arrays for features and target variable\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "\n",
    "# Splitting the data into training and testing datasets\n",
    "Xtrain, Xtest = X[:70], X[70:] #First dataset contain the first 70 rows, remaining uses the rest\n",
    "ytrain, ytest = y[:70], y[70:] #First dataset contain the first 70 rows, remaining uses the rest\n",
    "\n",
    "# Plotting a pairplot for the filtered iris dataset\n",
    "#sns.pairplot(iris, hue=\"species\", palette=\"husl\", diag_kws = {'shade': False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.3:** Write a function which initiate a set of weights `W` with *length* that is one larger than the number of features in your data. Ensure that your initial weights are not exactly 0, but close to it. \n",
    ">\n",
    ">> _Hint 1:_ Use [np.random.RandomState](https://numpy.org/doc/1.16/reference/generated/numpy.random.RandomState.html) to set up a random number generator from which you can draw from a normal with mean 0 and scale 0.01. \n",
    ">\n",
    ">> _Hint 2:_ Say you have stored the random number generator in an object called `rgen`. You can then call `rgen.normal(size = 1 + columns_in_X)` to get the weights you want. You might want to tweak the `scale` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e3b503614aff204d3f433856c18db54",
     "grade": false,
     "grade_id": "cell-2f566125986ebf20",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#Function that initiate weights W, which is equivalent to the coefficients in econometrics.\n",
    "def initialize_weights(X):\n",
    "    rgen = np.random.RandomState(seed=666)\n",
    "    W = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "    return W\n",
    "\n",
    "# Initialize weights for Xtrain\n",
    "W = initialize_weights(Xtrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.4:** In this problem, you need to write two functions:\n",
    "> * `net_input(X, W)`: calculates _and returns_ the net-input, i.e the linear combination of features and weights, $z=w_0 + \\sum_k x_{k} w_{k}$\n",
    "> * `predict(X, W)`: a step function which returns 1 if the net activation is $\\geq$ 0, and returns -1 otherwise. \n",
    ">\n",
    ">*Bonus:* Create a function which calculates the _accuracy_ (the share of cases that are correctly classified). The function should take a vector of y-values and a vector of predicted y-values as input. What is the accuracy of your untrained model on the training data?\n",
    "\n",
    "> _Hint 1:_ you can compute the above using an array product. Here numpy's array product named `dot` may be useful \n",
    "\n",
    "> _Hint 2:_ remember to include the bias, $w_0$, in the computation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of the code below**\n",
    "- net_input(X, W) computes the linear combination of the **features (independant variables)** and **weights (coefficients)**.\n",
    "- predict(X, W) uses the above function to get the **net input (weighted sum)** and then applies a step function to predict class labels **Over or equal to 0, set to 1, otherwise set to -1**\n",
    "- accuracy(y_true, y_pred) computes the accuracy of the model by comparing true class labels with predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad5e3997c561b73c4f881d23f72cc33a",
     "grade": false,
     "grade_id": "cell-9a4c65944f10d113",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#Checking accuracy of model on the training data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Predictions using untrained weights\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict(Xtrain, \u001b[43mweights\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[0;32m     22\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(ytrain, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def net_input(X, W):\n",
    "    \"\"\"Calculate and return the net-input (linear combination of features and weights)\"\"\"\n",
    "    return np.dot(X, W[1:]) + W[0] #np.dot returns cross product\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"Return class label after applying step function\"\"\"\n",
    "    return np.where(net_input(X, W) >= 0.0, 1, -1)\n",
    "\n",
    "#Bonus\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate the accuracy of the model\"\"\"\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "#Checking accuracy of model on the training data\n",
    "# Predictions using untrained weights\n",
    "y_pred = predict(Xtrain, weights)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy(ytrain, y_pred)\n",
    "print(f\"Accuracy of the untrained model on the training data: {acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.5:** Write a function whichs loops over the training data (both X and y) using `zip`. For each row in the data, update the weights according to the perceptron rule (remember to update the bias in `w[0]`!). Set $\\eta = 0.1$.\n",
    ">\n",
    "> Make sure the loop stores the total number of prediction errors encountered underways in the loop by creating an `int` which is incremented whenever you update the weights. \n",
    ">\n",
    ">> _Hint:_ your function should return the updated weights, as well as the number of errors made by the perceptron.\n",
    ">\n",
    ">> _Hint:_ The following code block implements the function in _pseudo_code (it wont run, but serves to communicate the functionality).\n",
    ">> ```\n",
    ">> function f(X, y, W, eta):\n",
    ">>    set errors = 0\n",
    ">>\n",
    ">>    for each pair xi, yi in zip(X,y) do:\n",
    ">>        set update = eta * (yi - predict(xi, W))\n",
    ">>        set W[1:] = W[1:] + update * xi\n",
    ">>        set W[0] = W[0] + update\n",
    ">>        set errors = errors + int(update != 0) \n",
    ">>\n",
    ">>    return W, errors\n",
    ">> ```\n",
    ">\n",
    "> *Bonus:* If you completed the previous bonus exercise (for 10.1.4), calculate the accuracy on training data using the updated weights as input in the predict function. Any progress yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eaaaa47b55e584c64d865d8e5b7d751",
     "grade": false,
     "grade_id": "cell-67d2591f252b5e1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def perceptron_update(X, y, W, eta=0.1):\n",
    "    errors = 0\n",
    "    \n",
    "    for xi, yi in zip(X, y):\n",
    "        update = eta * (yi - predict(xi, W)) \n",
    "        W[1:] += update * xi\n",
    "        W[0] += update\n",
    "        errors += int(update != 0.0)\n",
    "    \n",
    "    return W, errors\n",
    "\n",
    "# Bonus: Calculate the accuracy on training data using the updated weights\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Example usage after defining the above functions:\n",
    "# Assuming you've already initialized your weights and loaded your data into Xtrain and ytrain\n",
    "\n",
    "W, initial_errors = perceptron_update(Xtrain, ytrain, W)\n",
    "print(f\"Number of errors in initial epoch: {initial_errors}\")\n",
    "\n",
    "# Bonus: Calculate accuracy\n",
    "y_pred = [predict(xi, W) for xi in Xtrain]\n",
    "acc = accuracy(ytrain, y_pred)\n",
    "print(f\"Accuracy on training data: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.6:** Write a function, which repeats the updating procedure (calls the function) you constructed in 10.1.5 for `n_iter` times by packing the whole thing in a loop. Make sure you store the number of errors in each iteration in a list. \n",
    ">\n",
    "> Plot the total errors after each iteration in a graph.\n",
    ">\n",
    "> _Hint 1:_ Make sure you dont reset the weights after each iteration.\n",
    ">\n",
    "> _Hint 2:_ Once again some pseudocode:\n",
    ">> ```\n",
    ">> function g(X, y, n_iter):\n",
    ">>     set eta = 0.1\n",
    ">>     set weights = random_weights()\n",
    ">>     set errorseq = list()\n",
    ">>\n",
    ">>     for each _ in range(n_iter):\n",
    ">>         weights, e = f(X, y, weights, eta) \n",
    ">>         errorseq.append(e)\n",
    ">>\n",
    ">>     return weights, errorseq\n",
    ">> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the below code**\n",
    "- This function trains the perceptron for a specified number of iterations (n_iter). After training, the function will return the final weights and a list of the number of errors for each iteration. The plot displays how the number of errors changes over each epoch, which can be useful to see if and when the model converges to a solution (i.e., when errors stabilize or reduce to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "724329828010bf30230618b24583db0b",
     "grade": false,
     "grade_id": "cell-059f1639214a2ed7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_train(X, y, n_iter=10):\n",
    "    eta = 0.1\n",
    "    weights = random_weights(X.shape[1])  # random_weights has been defined already\n",
    "    errorseq = []\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        weights, e = perceptron_update(X, y, weights, eta)\n",
    "        errorseq.append(e)\n",
    "\n",
    "    return weights, errorseq\n",
    "\n",
    "# Train the perceptron and get the weight updates over epochs\n",
    "final_weights, errors = perceptron_train(Xtrain, ytrain, n_iter=100)\n",
    "\n",
    "# Plotting the errors\n",
    "plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates/errors')\n",
    "plt.title('Perceptron Training: Errors vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.7 (BONUS):** Use the updated weights when predicting and calculate the accuracy of your perceptron on the test data?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of 10.1.7**\n",
    "- Use the updated weights from your training (which you obtained after running through multiple epochs) to make predictions on your test data.\n",
    "- Calculate the accuracy of these predictions by comparing them to the actual test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12285bf4fc19c433a34e05d3eeb850b8",
     "grade": false,
     "grade_id": "cell-e39721b77e2e76f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "y_pred = predict(Xtest, final_weights)\n",
    "test_accuracy = accuracy(ytest, y_pred)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Beyond the Perceptron Model\n",
    "\n",
    "Having seen and worked with the perceptron, we now want to provide you with some ideas on how we can change parts of the perceptron to obtain another model. In particular, we will introduce the _logistic regression model_ and the _adaline (adaptive linear neuron) model_. \n",
    "\n",
    "In practice, these models distinguish themselves along particularly two margins. \n",
    "- First, in the standard implementation, weight updating is typically conducted using (a batch of the) full training data set - rather than one observation at a time. \n",
    "- Furthermore, weight updating essentially accounts for the _degree_ at which we are making a wrong prediction. \n",
    "    - If we are very certain that observation _i_ is the positive type, when it is in fact negative, we will treat it differently from an observation that we are not very certain of when misclassifying. \n",
    "    - As we will see, logistic regressions, is also based on a notion of _conditional probabilites_ (i.e. \"what is the probability that _i_ is the positive type when _x_ is equal to 5), when while Adaline is not.\n",
    "\n",
    "*Note:* Again, you may want to familiarize yourself with background concepts: [gradient](https://en.wikipedia.org/wiki/Gradient), [sum of squared errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares) and the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('wPGctoIX72c', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is another simple linear machine-learning algorithm, you can read about it [here:](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.0:** Import the LogisticRegression classifier from `sklearn.linear_model`. Create a new object called `clf` like:\n",
    "> ```\n",
    "> clf = LogisticRegression()\n",
    "> ```\n",
    "All scikit learn models have two fundamental methods `.fit()` and `.predict()`. Fit your model to the training data, and store the fitted model in a new object. Import _accuracy_score_ from `sklearn.metrics` and asses the accuracy of the LogisticRegression on both your training data and your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e245ccf10e5687f828736146707bb51",
     "grade": false,
     "grade_id": "cell-64462b79a8d3cc8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "ytrain_pred = clf.predict(Xtrain)\n",
    "ytest_pred = clf.predict(Xtest)\n",
    "\n",
    "train_accuracy = accuracy_score(ytrain, ytrain_pred)\n",
    "test_accuracy = accuracy_score(ytest, ytest_pred)\n",
    "\n",
    "print(f\"Accuracy on training data: {train_accuracy:.4f}\")\n",
    "print(f\"Accuracy on test data: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaLine\n",
    "As we saw in the video, AdaLine is a modified version of the perceptron. The most important difference lies in the way the two models learn from their training data, i.e. the optimization method used. The perceptron used the binary classifications for learning, while AdaLine only applies the binary threshold after training, and thus uses real valued numbers when learning. Another difference is that in the standard implementation, the weight update in Adaline is conducted using the full set of data or a subset of data.\n",
    "\n",
    "> _Hint:_ In this set of exercises, most of the code for the following exercise can be written by copying and modifying code from previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.1:** Implement two functions as described below. You shold reuse your `net_input` from previous exercises:\n",
    "> * `ada_activation_function`: the linear function $ada\\_activation(z) = z$\n",
    "> * `ada_predict`: A step function   $ada\\_predict(z) = 1 \\ if \\ z \\geq 0  \\ else \\ 0$ where z is the output of _the activation function_.\n",
    "\n",
    "\n",
    "\n",
    "> The following figure might help you understand how each of these functions relate to the algorithm, and how the perceptron and adaline differ:\n",
    "![asd](https://sebastianraschka.com/images/faq/diff-perceptron-adaline-neuralnet/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "- ada_activation_function: This function simply returns the input value, as the activation function for Adaline is the identity function.\n",
    "- ada_predict: This function returns 1 if the input value (which is the output of the activation function) is greater than or equal to 0, else it returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60e7fb372d0acaba9f87c5708fdd1231",
     "grade": false,
     "grade_id": "cell-8e63b21a623d6fda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def ada_activation_function(z):\n",
    "    \"\"\"\n",
    "    The Adaline activation function (identity function).\n",
    "    \n",
    "    Parameters:\n",
    "    - z : Net input (output from net_input function)\n",
    "\n",
    "    Returns:\n",
    "    - z : The same input value\n",
    "    \"\"\"\n",
    "    return z\n",
    "\n",
    "def ada_predict(z):\n",
    "    \"\"\"\n",
    "    Predicts the class label using the step function.\n",
    "    \n",
    "    Parameters:\n",
    "    - z : Output of the activation function\n",
    "\n",
    "    Returns:\n",
    "    - 1 if z >= 0, else 0\n",
    "    \"\"\"\n",
    "    return 1 if z >= 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.2:** AdaLine uses a _cost function_ to quantize the accuracy of the classifier this is given by \n",
    ">$$ \n",
    "cost(X,y,W) = \\frac{1}{2} \\sum_{i=1}^N (y_i - activation(z_i) )^2 , \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    "> If you've followed any normal undergraduate courses in statistics you should recognize this function. Begin by implementing the cost function. Unlike in undergraduate statistics we will optimize our estimator using gradient descent, therefore **code up the negative of the derivative of the cost function as well**. \n",
    "> $$ \n",
    "-cost'_j(X,y, W) = -\\sum_{i=1}^N (y_i - activation(z_i)) x_i^j,  \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    ">\n",
    ">> _Hint:_ Dont compute the sum for each weight $w_j$, instead use numpy's matrix algebra to compute the all of the derivatives at once.\n",
    ">\n",
    ">> _Hint:_ The derivative should return a list of the same length as the number of weights, since there is one derivative for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c55fe6efee197cdf1e28df4214527593",
     "grade": false,
     "grade_id": "cell-b5b1cd22ca65a2fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def cost_function(X, y, W):\n",
    "    \"\"\"Computes the cost function for Adaline.\"\"\"\n",
    "    z = net_input(X, W)\n",
    "    errors = (y - ada_activation_function(z))\n",
    "    cost = 0.5 * np.sum(errors**2)\n",
    "    return cost\n",
    "\n",
    "def cost_derivative(X, y, W):\n",
    "    \"\"\"Computes the negative of the derivative of the cost function.\"\"\"\n",
    "    z = net_input(X, W)\n",
    "    errors = (y - ada_activation_function(z))\n",
    "    return -X.T.dot(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.3:** Implement the adaline fitting algorithm using *batch gradient descent*. This is similar to what you did with the perceptron, but while the perceptron did it's optimization after evaluating each row in the dataset, adaline treats the entire dataset as a batch, adjusts it's weights and then does it all again. Thus you only need to loop over `n_iter`, _not_ the data rows. Use the cost function to track the progress of your algorithm.\n",
    ">\n",
    "> _Hint:_ gradient descent will be extremely sensitive to the learning rate $\\eta$ in this situation - try setting i to 0.0001 and running the algorithm for 5000 iterations to get some kind of convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a154b0507f95fcc55c65ad42be6de8",
     "grade": false,
     "grade_id": "cell-ce729986f2f1b230",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def adaline_fit(X, y, n_iter=5000, eta=0.0001):\n",
    "    \"\"\"Fits data to Adaline using batch gradient descent.\"\"\"\n",
    "    weights = random_weights(X.shape[1])\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        gradient = cost_derivative(X, y, weights)\n",
    "        weights -= eta * gradient\n",
    "        cost_history.append(cost_function(X, y, weights))\n",
    "    \n",
    "    return weights, cost_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.4:** Write a function that scales each of the variables in the dataset (including **y**) using the formula \n",
    "$$\n",
    "x_j^{new} = \\frac{x_j^{old} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "> rerun the adaline function on the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56e621f7a56e3efef9a18da453ea6ea9",
     "grade": false,
     "grade_id": "cell-4698ea41d9f80069",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def scale_features(data):\n",
    "    \"\"\"Scales features of the dataset.\"\"\"\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scale_features(X)\n",
    "y_scaled = scale_features(y)\n",
    "\n",
    "weights_scaled, cost_history_scaled = adaline_fit(X_scaled, y_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
