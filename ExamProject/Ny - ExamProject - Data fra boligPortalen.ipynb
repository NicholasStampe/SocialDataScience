{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Project - Scraping BoligPortalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages:\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Import packages:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Define function to get the data from the website: BoligPortal \n",
    "def scrape_data(offset):\n",
    "    # Define the base URL:\n",
    "    base_url = 'https://www.boligportal.dk/lejeboliger/?min_rental_period=0&offset={}'\n",
    "\n",
    "    # Create the complete URL with the given offset:\n",
    "    url = base_url.format(offset)\n",
    "\n",
    "    # Connect to site:\n",
    "    response = requests.get(url, headers={'name':'Jesper Højberg Knudsen','email':'fmw786@econ.ku.dk'})\n",
    "\n",
    "    # Parse data with BeautifulSoup:\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    # Find all links to the individual ads:\n",
    "    links = soup.find_all('a', class_='AdCardSrp__Link css-17x8ssx')\n",
    "\n",
    "    # Make a list of URLs:\n",
    "    url_list = [link['href'] for link in links]\n",
    "\n",
    "    # Return the 4th to 21st URLs (indices 3 to 20)\n",
    "    return url_list[3:21]\n",
    "\n",
    "# Set the initial offset and the step size:\n",
    "initial_offset = 0\n",
    "step_size = 18\n",
    "\n",
    "# Define the number of iterations you want to perform:\n",
    "num_iterations = 10  # You can change this as needed\n",
    "\n",
    "# Create an empty list to store all URLs:\n",
    "all_urls = []\n",
    "\n",
    "# Loop through the desired number of iterations:\n",
    "for i in tqdm(range(num_iterations), desc=\"Scraping URLs\"):  # Use tqdm for progress bar\n",
    "    offset = initial_offset + (i * step_size)\n",
    "    urls = scrape_data(offset)\n",
    "    all_urls.extend(urls)\n",
    "\n",
    "# Make final list of working URLs:\n",
    "final_urls = []\n",
    "\n",
    "for url in all_urls:\n",
    "    temp_url = 'https://www.boligportal.dk' + url\n",
    "    final_urls.append(temp_url)\n",
    "\n",
    "# Print all final scraped URLs:\n",
    "for url in final_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of information we want to scarpe:\n",
    "adress_list = []\n",
    "monthly_rent_list = []\n",
    "aconto_list = []\n",
    "sqm_list = []\n",
    "nr_rooms_list = []\n",
    "floor_list = []\n",
    "property_type_list = []\n",
    "furnished_list = []\n",
    "shareable_list = []\n",
    "pets_allowed_list = []\n",
    "elevator_list = []\n",
    "senior_frendly_list = []\n",
    "only_for_students_list = []\n",
    "balcony_or_terrace_list = []\n",
    "parking_list = []\n",
    "energy_label_list = []\n",
    "description_list = []\n",
    "\n",
    "# Initialize a counter to keep track of the number of scraped URLs\n",
    "scraped_count = 0\n",
    "\n",
    "# Initialize lists to hold data for the current chunk\n",
    "current_chunk = []\n",
    "\n",
    "# Loop through all the pages\n",
    "for i in tqdm(range(len(final_urls)), desc=\"Scraping Data\"):\n",
    "    # Scraping\n",
    "    url = final_urls[i]\n",
    "    response = requests.get(url, headers={'name':'Jesper Højberg Knudsen','email':'fmw786@econ.ku.dk'})\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Adress\n",
    "    # Find the label \"css-11fbmqw\" within the section\n",
    "    element = soup.find('span', class_='css-11fbmqw')\n",
    "    if element:\n",
    "        # Extract the corresponding value\n",
    "        adress_element = element.find_next('div', class_='css-v49nss')\n",
    "\n",
    "        if adress_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            adress_list.append(adress_element.text.strip())\n",
    "        else:\n",
    "            adress_list.append('N/A') # Handle case if the value is not found\n",
    "\n",
    "    else:\n",
    "        adress_list.append('N/A') # Handle case if the label is not found\n",
    "\n",
    "\n",
    "    # Monthly rent\n",
    "    # Find the label \"Månedlig leje\" within the section\n",
    "    label_element = soup.find('span', text='Månedlig leje', class_='css-arxwps')\n",
    "    \n",
    "    if label_element:\n",
    "        # Extract the corresponding value\n",
    "        monthly_rent_element = label_element.find_next('span', class_='css-1h46kg2')\n",
    "        \n",
    "        if monthly_rent_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            monthly_rent_list.append(monthly_rent_element.text.strip())\n",
    "        else:\n",
    "            monthly_rent_list.append('N/A')  # Handle case if the value is not found\n",
    "    else:\n",
    "        monthly_rent_list.append('N/A')  # Handle case if the label is not found\n",
    "\n",
    "    # Aconto\n",
    "    # Find the label \"Aconto\" within the section\n",
    "    aconto_element = soup.find('span', class_='css-arxwps', text='Aconto')\n",
    "\n",
    "    if aconto_element:\n",
    "        # Extract the corresponding value\n",
    "        aconto_element = aconto_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if aconto_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            aconto_list.append(aconto_element.text.strip())\n",
    "        else:\n",
    "            aconto_list.append('N/A')\n",
    "    else:\n",
    "        aconto_list.append('N/A')\n",
    "        \n",
    "    # Square meters\n",
    "    # Find the label \"Størrelse\" within the section\n",
    "    square_element = soup.find('span', class_='css-arxwps', text='Størrelse')\n",
    "\n",
    "    if square_element:\n",
    "        # Extract the corresponding value\n",
    "        square_element = square_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if square_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            sqm_list.append(square_element.text.strip())\n",
    "        else:\n",
    "            sqm_list.append('N/A')\n",
    "    else:\n",
    "        sqm_list.append('N/A')\n",
    "\n",
    "\n",
    "    # Number of rooms\n",
    "    # Find the label \"Værelser\" within the section\n",
    "    nr_rooms_element = soup.find('span', class_='css-arxwps', text='Værelser')\n",
    "\n",
    "    if nr_rooms_element:\n",
    "        # Extract the corresponding value\n",
    "        nr_rooms_element = nr_rooms_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if nr_rooms_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            nr_rooms_list.append(nr_rooms_element.text.strip())\n",
    "        else:\n",
    "            nr_rooms_list.append('N/A')\n",
    "    else:\n",
    "        nr_rooms_list.append('N/A')\n",
    "    \n",
    "    # Floor\n",
    "    # Find the label \"Etage\" within the section\n",
    "    floor_element = soup.find('span', class_='css-arxwps', text='Etage')\n",
    "\n",
    "    if floor_element:\n",
    "        # Extract the corresponding value\n",
    "        floor_element = floor_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if floor_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            floor_list.append(floor_element.text.strip())\n",
    "        else:\n",
    "            floor_list.append('N/A')\n",
    "    else:\n",
    "        floor_list.append('N/A')\n",
    "\n",
    "    # Property type\n",
    "    # Find the label \"Boligtype\" within the section\n",
    "    property_type_element = soup.find('span', class_='css-arxwps', text='Boligtype')\n",
    "    if property_type_element:\n",
    "        # Extract the corresponding value\n",
    "        property_type = property_type_element.find_next('span', class_='css-1h46kg2')\n",
    "        if property_type:\n",
    "            # Extract the text content and add it to the list\n",
    "            property_type_list.append(property_type.text.strip())\n",
    "        else:\n",
    "            property_type_list.append('N/A')\n",
    "    else:\n",
    "        property_type_list.append('N/A')\n",
    "\n",
    "    # Furnished\n",
    "    # Find the label \"Møbleret\" within the section\n",
    "    furnished_element = soup.find('span', class_='css-arxwps', text='Møbleret')\n",
    "\n",
    "    if furnished_element:\n",
    "        # Extract the corresponding value\n",
    "        furnished_element = furnished_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if furnished_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            furnished_list.append(furnished_element.text.strip())\n",
    "        else:\n",
    "            furnished_list.append('N/A')\n",
    "    else:\n",
    "        furnished_list.append('N/A')\n",
    "\n",
    "    # Shareable\n",
    "    # Find the label \"Delevenlig\" within the section\n",
    "    shareable_element = soup.find('span', class_='css-arxwps', text='Delevenlig')\n",
    "\n",
    "    if shareable_element:\n",
    "        # Extract the corresponding value\n",
    "        shareable_element = shareable_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if shareable_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            shareable_list.append(shareable_element.text.strip())\n",
    "        else:\n",
    "            shareable_list.append('N/A')\n",
    "    else:\n",
    "        shareable_list.append('N/A')\n",
    "\n",
    "    # Pets allowed\n",
    "    # Find the label \"Husdyr tilladt\" within the section\n",
    "    pets_allowed_element = soup.find('span', class_='css-arxwps', text='Husdyr tilladt')\n",
    "\n",
    "    if pets_allowed_element:\n",
    "        # Extract the corresponding value\n",
    "        pets_allowed_element = pets_allowed_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if pets_allowed_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            pets_allowed_list.append(pets_allowed_element.text.strip())\n",
    "        else:\n",
    "            pets_allowed_list.append('N/A')\n",
    "    else:\n",
    "        pets_allowed_list.append('N/A')\n",
    "\n",
    "    # elevator\n",
    "    # Find the label \"Elevator\" within the section\n",
    "    elevator_element = soup.find('span', class_='css-arxwps', text='Elevator')\n",
    "\n",
    "    if elevator_element:\n",
    "        # Extract the corresponding value\n",
    "        elevator_element = elevator_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if elevator_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            elevator_list.append(elevator_element.text.strip())\n",
    "        else:\n",
    "            elevator_list.append('N/A')\n",
    "    else:\n",
    "        elevator_list.append('N/A')\n",
    "\n",
    "    # Seniors frendly\n",
    "    # Find the label \"Seniorvenlig\" within the section\n",
    "    seniors_frendly_element = soup.find('span', class_='css-arxwps', text='Seniorvenlig')\n",
    "\n",
    "    if seniors_frendly_element:\n",
    "        # Extract the corresponding value\n",
    "        seniors_frendly_element = seniors_frendly_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if seniors_frendly_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            senior_frendly_list.append(seniors_frendly_element.text.strip())\n",
    "        else:\n",
    "            senior_frendly_list.append('N/A')\n",
    "    else:\n",
    "        senior_frendly_list.append('N/A')\n",
    "\n",
    "    # Students only\n",
    "    students_only_element = soup.find('span', class_='css-arxwps', text='Kun for studerende')\n",
    "\n",
    "    if students_only_element:\n",
    "        # Extract the corresponding value\n",
    "        students_only_element = students_only_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if students_only_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            only_for_students_list.append(students_only_element.text.strip())\n",
    "        else:\n",
    "            only_for_students_list.append('N/A')\n",
    "    else:\n",
    "        only_for_students_list.append('N/A')\n",
    "\n",
    "        \n",
    "    # Balcony or terrace\n",
    "    balcony_element = soup.find('span', class_='css-arxwps', text='Altan/terrasse')\n",
    "\n",
    "    if balcony_element:\n",
    "        # Extract the corresponding value\n",
    "        balcony_element = balcony_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if balcony_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            balcony_or_terrace_list.append(balcony_element.text.strip())\n",
    "        else:\n",
    "            balcony_or_terrace_list.append('N/A')\n",
    "    else:\n",
    "        balcony_or_terrace_list.append('N/A')\n",
    "\n",
    "    # Parking\n",
    "    parking_element = soup.find('span', class_='css-arxwps', text='Parkering')\n",
    "\n",
    "    if parking_element:\n",
    "        # Extract the corresponding value\n",
    "        parking_element = parking_element.find_next('span', class_='css-1h46kg2')\n",
    "\n",
    "        if parking_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            parking_list.append(parking_element.text.strip())\n",
    "        else:\n",
    "            parking_list.append('N/A')\n",
    "    else:\n",
    "        parking_list.append('N/A')\n",
    "\n",
    "    # Energy label\n",
    "    # Find: <img src=\"/static/images/energy_labels/C_str2.png\" class=\"css-rdsunt\">\n",
    "    # Get \"C_str2\" from the src attribute\n",
    "    energy_label_element = soup.find('span', class_='css-arxwps', text='Energimærke')\n",
    "    energy_label_list1 = []\n",
    "\n",
    "    if energy_label_element:\n",
    "        # Extract the corresponding value\n",
    "        energy_label_element = energy_label_element.find_next('img', class_='css-rdsunt')\n",
    "\n",
    "        if energy_label_element:\n",
    "            # Extract the text content and add it to the list\n",
    "            energy_label_list1.append(energy_label_element['src'].split('/')[-1].split('_')[0])\n",
    "            # Get the first letter of C_str2\n",
    "            energy_label_list.append(energy_label_list1[0][0])\n",
    "\n",
    "        else:\n",
    "            energy_label_list.append('N/A')\n",
    "    else:\n",
    "        energy_label_list.append('N/A')\n",
    "\n",
    "    # Find the div with class \"css-1f7mpex\" for the description\n",
    "    description_element = soup.find('div', class_='css-1f7mpex')\n",
    "\n",
    "    # Initialize an empty string to store the description\n",
    "    description_text = \"\"\n",
    "\n",
    "    # Check if the description element is found\n",
    "    if description_element:\n",
    "        # Extract the text content of the description\n",
    "        description_text = description_element.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # If description_text is empty, set it to \"N/A\"\n",
    "    if not description_text:\n",
    "        description_text = \"N/A\"\n",
    "\n",
    "    # Add the description text to the list\n",
    "    description_list.append(description_text)\n",
    "\n",
    "    # Append data to the current chunk\n",
    "    current_chunk.append({\n",
    "        'Adress': adress_list[i],\n",
    "        'Monthly rent': monthly_rent_list[i],\n",
    "        'Aconto': aconto_list[i],\n",
    "        'Square meters': sqm_list[i],\n",
    "        'Rooms': nr_rooms_list[i],\n",
    "        'Floor': floor_list[i],\n",
    "        'Property type': property_type_list[i],\n",
    "        'Furnished': furnished_list[i],\n",
    "        'Shareable': shareable_list[i],\n",
    "        'Pets allowed': pets_allowed_list[i],\n",
    "        'Elevator': elevator_list[i],\n",
    "        'Senior frendly': senior_frendly_list[i],\n",
    "        'Only for students': only_for_students_list[i],\n",
    "        'Balcony/Terrace': balcony_or_terrace_list[i],\n",
    "        'Parking': parking_list[i],\n",
    "        'Energy label': energy_label_list[i],\n",
    "        'Description': description_list[i],\n",
    "        'Link': final_urls[i]\n",
    "    })\n",
    "\n",
    "    # Increment the counter\n",
    "    scraped_count += 1\n",
    "\n",
    "    # Check if 100 URLs have been scraped or if it's the last iteration, and save the dataframe\n",
    "    if scraped_count % 100 == 0 or i == len(final_urls) - 1:\n",
    "        chunk_number = (scraped_count - 1) // 100 + 1  # Update chunk number calculation\n",
    "        \n",
    "        # Check if it's the last iteration and adjust the data for the current chunk accordingly\n",
    "        if i == len(final_urls) - 1:\n",
    "            current_chunk = current_chunk[:scraped_count % 100 if scraped_count % 100 != 0 else 100]\n",
    "        \n",
    "        df_chunk = pd.DataFrame(current_chunk)\n",
    "\n",
    "        # Save the dataframe\n",
    "        df_chunk.to_csv(f'data_chunk_{chunk_number}.csv', index=False)\n",
    "\n",
    "        # Clear the current chunk for the next iteration\n",
    "        current_chunk = []\n",
    "\n",
    "    # Pause for a short time before scraping the next page\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Concatenate and save the final DataFrame\n",
    "all_chunks = []\n",
    "num_chunks = (scraped_count // 100) + (1 if scraped_count % 100 != 0 else 0)\n",
    "\n",
    "for chunk_number in range(1, num_chunks + 1):\n",
    "    chunk_filename = f'data_chunk_{chunk_number}.csv'\n",
    "    chunk_df = pd.read_csv(chunk_filename)\n",
    "    all_chunks.append(chunk_df)\n",
    "\n",
    "# Concatenate all chunks\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Save the final dataframe\n",
    "final_df.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved CSV file and display its contents\n",
    "boligportalen_df = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Remove the duplicated links from the dataframe\n",
    "boligportalen_df = boligportalen_df.drop_duplicates(subset=['Link'])\n",
    "\n",
    "# Save boligportalen_df as a csv file\n",
    "boligportalen_df.to_csv('boligportalen_df.csv', index=False)\n",
    "\n",
    "boligportalen_df.head(1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
