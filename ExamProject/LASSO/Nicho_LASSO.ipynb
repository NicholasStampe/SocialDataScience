{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Videos and Exercises for Session 11: Regression and Regularization\n",
    "\n",
    "In this combined teaching module and exercise set, you will learn about linear regression models in a machine learning perspective. We will see how overfitting can arise and how we can tackle it with a modification of the linear regression model.\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "1. Linear Regression Mechanics\n",
    "2. Overfitting and Underfitting in Linear Regression\n",
    "    - Exploring Overfitting in Linear Regression\n",
    "    - A Cure for Overfitting in Linear Regression\n",
    "3. Modelling Houseprices (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "First, we need to import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn, so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression Mechanics\n",
    "## Implementing and evaluating the gradient decent \n",
    " \n",
    "Normally we use OLS to estimate linear regression models, but this is only way of solving the problem of minimizing the least squares problem (that minimizes the sum of squared errors). In the video below we show how to implement gradient descent below and compare it along with other approximate solutions to OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overblik over Exercise 11.1 og formÃ¥let med opgaven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ex. 11.1.0: Importing Data:\n",
    "- - Purpose: To get comfortable with loading datasets using libraries like Seaborn. Loading and exploring data is the first step in any data analysis task.\n",
    "- Ex. 11.1.1: Data Preprocessing:\n",
    "- - Purpose: Understand how to prepare data by converting categorical variables into a format suitable for machine learning. This step ensures the dataset is in the right shape for model training.\n",
    "- Ex. 11.1.2: Train-Test Split:\n",
    "- - Purpose: Recognize the importance of splitting data into training and testing subsets. This allows for model validation and helps prevent overfitting.\n",
    "- Ex. 11.1.3: Feature Normalization:\n",
    "- - Purpose: Grasp the importance of scaling features so that they have a similar scale. It helps algorithms converge faster and leads to better model performance.\n",
    "- Ex. 11.1.4: Compute Error:\n",
    "- - Purpose: Understand the concept of prediction error. It's the difference between the predicted and actual values. Reducing this error is the main goal of training a machine learning model.\n",
    "- Ex. 11.1.5: Weight Update Mechanism:\n",
    "- - Purpose: Introduce the core logic of gradient descent. Here, the weight of each feature is adjusted iteratively to minimize the prediction error.\n",
    "- Ex. 11.1.6: Weight Initialization and Iteration:\n",
    "- - Purpose: Understand the importance of weight initialization and how iterative updates (using the logic from the previous exercise) help in model optimization.\n",
    "- Ex. 11.1.7: Model Validation and Visualization:\n",
    "- - Purpose: Validate the custom-built gradient descent solution against a standard library implementation, ensuring its correctness. Visualizing the error helps understand how well the model is performing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue straight to an exercise where you are to implement a new estimator that we code up from scratch. We solve the numerical optimization using the gradient decent algorithm. This will be very similar to what we just saw in the video, but we will pay a bit more attention to each step in the process.\n",
    "\n",
    "Using our algorithm, we will fit it to some data, and compare our own solution to the standard solution from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.0**: Import the dataset `tips` from the `seaborn`.\n",
    "\n",
    "\n",
    "*Hint*: use the `load_dataset` method in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e812556b5b6774ba76a4a1ccb89a169",
     "grade": false,
     "grade_id": "cell-2dd56f36f76bce57",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV file from your computer\n",
    "Data = pd.read_csv('4_Final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the first two columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.1**:Restructure the data so we get a dataset `y` containing the variable tip, and a dataset `X` containing the \n",
    "features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ce55b897e3acd6727916dffe21150f6",
     "grade": false,
     "grade_id": "cell-2e42eb4f59160bed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X = Data.drop(['Monthly rent', 'Longitude', 'Latitude', 'PostalCode_1000-1999', 'Floor_-1 to 0', 'Distance to Transport Station (km)', 'Distance to Beach (km)', 'Distance to School (km)', 'Distance to Restaurant (km)', 'Distance to Hospital (km)', 'Distance to Mall (km)'], axis=1)\n",
    "y = Data['Monthly rent']  # Labels (the 'Monthly rent' column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.2**: Divide the features and target into test and train data. Make the split 50 pct. of each. The split data should be called `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    "> *Hint*: You may use `train_test_split` in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb3675bb8e21477c6f5c76f67a3a5ed0",
     "grade": false,
     "grade_id": "cell-ba197171f1b2bfef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.3**: Normalize your features by converting to zero mean and one std. deviation.\n",
    "\n",
    "> *Hint*: Take a look at `StandardScaler` in `sklearn.preprocessing`. If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a4d5e2ed6417f454e18385a2804a19b",
     "grade": false,
     "grade_id": "cell-8ab591d5927be1d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.4**: Make a function called `compute_error` to compute the prediction errors given input target `y_`, input features `X_` and input weights `w_`. You should use matrix multiplication.\n",
    ">\n",
    "> *Hint:* You can use the net-input fct. from yesterday.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b03c6de7d3488e832c4d3eb123587e17",
     "grade": false,
     "grade_id": "cell-a70101715bbbb443",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def compute_error(y_, X_, w_):\n",
    "    \"\"\"Compute the prediction errors.\"\"\"\n",
    "    predictions = X_.dot(w_)\n",
    "    errors = y_ - predictions\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.5**: Make a function to update the weights given input target `y_`, input features `X_` and input weights `w_` as well as learning rate, $\\eta$, i.e. greek `eta`. You should use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd45bb01781e944c41227e59873a6df",
     "grade": false,
     "grade_id": "cell-049443f1aafb8903",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def update_weights(y_, X_, w_, eta):\n",
    "    # Compute the prediction errors\n",
    "    errors = y_ - np.dot(X_, w_[1:]) - w_[0]\n",
    "    \n",
    "    # Update the weights using the gradient descent formula\n",
    "    w_[1:] += eta * X_.T.dot(errors)\n",
    "    w_[0] += eta * errors.sum()\n",
    "    \n",
    "    return w_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.6**: Use the code below to initialize weights `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X_train.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6308f5593a6e65ab57b86368ceef6669",
     "grade": false,
     "grade_id": "cell-74c4170d3d5fe322",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "w = np.zeros(1+X_train.shape[1]) # Initialize weights\n",
    "eta = 0.001 # Learning rate\n",
    "\n",
    "# Iteratively update weights\n",
    "for _ in range(50):\n",
    "    w = update_weights(y_train, X_train, w, eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.7**: Make a function to compute the mean squared error. Alter the loop so it makes 100 iterations and computes the MSE for test and train after each iteration, plot these in one figure. \n",
    "\n",
    "> Hint: You can use the following code to check that your model works:\n",
    ">```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "assert((w[1:] - reg.coef_).sum() < 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code updates the weights over 100 iterations, computes the MSE for both the training and test datasets at each iteration, and finally plots these MSE values using the pd.Series(...).plot() commands you mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "863ac125b673cb711cfc5269a05d1bf5",
     "grade": false,
     "grade_id": "cell-5cabc75ac6152434",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Function to compute mean squared error\n",
    "def compute_mse(y, X, w):\n",
    "    # Add a column of ones to X for the bias term\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    # Compute the predictions\n",
    "    y_pred = np.dot(X, w)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = ((y - y_pred) ** 2).mean()\n",
    "    \n",
    "    return mse\n",
    "\n",
    "w = np.zeros(1+X_train.shape[1]) # Initialize weights\n",
    "eta = 0.001 # Learning rate\n",
    "\n",
    "MSE_train = []  # list to store MSE values for training data\n",
    "MSE_test = []   # list to store MSE values for testing data\n",
    "\n",
    "# Iteratively update weights\n",
    "for _ in range(100):\n",
    "    w = update_weights(y_train, X_train, w, eta)\n",
    "    MSE_train.append(compute_mse(y_train, X_train, w))\n",
    "    MSE_test.append(compute_mse(y_test, X_test, w))\n",
    "\n",
    "# Plotting the MSE values using pandas Series\n",
    "pd.Series(MSE_train).plot(label='Train MSE')\n",
    "pd.Series(MSE_test).plot(label='Test MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('Mean Squared Error over iterations')\n",
    "plt.show()\n",
    "\n",
    "pd.Series(MSE_train).plot()\n",
    "pd.Series(MSE_test).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Overfitting and Underfitting in Linear Regression \n",
    "\n",
    "## Exploring Overfitting in Linear Regression\n",
    "How does overfitting manifest itself in linear regression? In the video below we simulate what happens as make a better and better taylor approximation, i.e. we estimate a polynomial of higher and higher order. Two issues arise simultaneously - one is related to the number of parameters and the to the size of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b4c075a4bad7561c1e03bf809f7faf",
     "grade": false,
     "grade_id": "cell-6aa146e9c530b2b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Modelling Houseprices\n",
    "In this example, we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.3.0:** Load the california housing data with scikit-learn using the code below. Now:\n",
    "> 1. Inspect *cal_house*. How are the data stored?\n",
    "> 2. Create a pandas DataFrame called *X*, using `data`. Name the columns using `feature_names`.\n",
    "> 3. Crate a pandas Series called *y* using `target`.\n",
    "> 4. Make a train test split of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08ca919735b17dfbf96058bf07f4eeab",
     "grade": false,
     "grade_id": "cell-5f14e576643ac94c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#cal_house = fetch_california_housing()\n",
    "\n",
    "# 1. The data in cal_house is stored as a Bunch object - similar to a dictionary.\n",
    "# 2. Create a pandas DataFrame for the features\n",
    "#X = pd.DataFrame(data=cal_house['data'], columns=cal_house['feature_names'])\n",
    "\n",
    "# 3. Create a pandas Series for the target variable\n",
    "#y = pd.Series(cal_house['target'])\n",
    "\n",
    "# 4. Make a train-test split of equal size\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex.11.3.1**: Generate interactions between all features to third degree (make sure you **exclude** the bias/intercept term). How many variables are there? Will OLS fail? After making interactions, rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    "\n",
    "> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "\n",
    "> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed00cd67cc7a2e431594d4cfe29da085",
     "grade": false,
     "grade_id": "cell-4aacfe9c22772c42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 275 variables after generating interactions up to second degree.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Generate 3rd degree polynomial features (excluding the bias/intercept term)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Number of variables\n",
    "num_vars = X_train_poly.shape[1]\n",
    "print(f\"There are {num_vars} variables after generating interactions up to second degree.\")\n",
    "\n",
    "# Rescale the features to have zero mean and unit std. deviation\n",
    "# We use the distribution of the training data to rescale the test data\n",
    "scaler = StandardScaler().fit(X_train_poly)\n",
    "X_train_poly_scaled = scaler.transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.11.3.2**: Estimate the Lasso model on the rescaled train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the rescaled test and train data. Take a look at the fitted coefficients for different sizes of $\\lambda$. What happens when $\\lambda$ increases? Why?\n",
    "\n",
    "> *Hint 1*: use `logspace` in numpy to create the range.\n",
    "\n",
    "> *Hint 2*: read about the `coef_` feature [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5cb040dc44fd9a9f591f1193a1311ab",
     "grade": false,
     "grade_id": "cell-d981c29cec05057b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing RMSE for Lasso:   1%|â                                                     | 1/100 [01:01<1:41:56, 61.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.0001\n",
      "Coefficients: [-9.66714394e+01  1.88678886e+03  1.28887371e+02  8.35801909e+01\n",
      " -1.15447765e+01  2.95239229e+02  1.52214396e+03  1.03543769e+03\n",
      "  2.71704274e+02  1.79498798e+02  2.02272686e+02  2.12823176e+02\n",
      " -3.86204953e+03  1.51631773e+03  7.13294986e+02  8.04031428e+02\n",
      "  1.08981132e+03  3.20539641e+02  2.45862233e+02  1.70375903e+02\n",
      " -6.34311326e+02 -2.97322465e+01 -1.91877426e+03 -2.04106437e+01\n",
      " -3.54124193e+02 -3.04913336e+02 -2.05133484e+02 -5.16451691e+01\n",
      "  3.88053704e+02 -1.87489525e+02  2.76122770e+01  6.86446970e+01\n",
      "  1.15635689e+01 -1.31593931e+01  1.26918292e+02 -1.18179989e+02\n",
      "  6.01712905e+01 -2.39043238e+01 -2.41790071e+01 -1.07934484e+02\n",
      " -9.61238736e+00 -9.31572802e+00 -1.35144101e+01 -5.33640874e+01\n",
      "  4.83968256e+00  6.14876406e+00  1.44335033e+01  1.95872417e+00\n",
      " -4.28796979e+01  3.79964489e+01  2.58092211e+01  8.51831463e+00\n",
      " -1.49712955e+01 -2.26843860e+01 -3.11279451e+02  1.13224047e+02\n",
      "  2.35156064e+02  1.54762790e+02 -7.78014117e+01  1.54148129e+02\n",
      " -8.33407886e+01  1.75122365e+02 -4.16171974e+02  1.19315599e+02\n",
      "  2.25317540e+02  3.75944527e+02  6.86435311e+02 -3.21690353e+01\n",
      " -1.60508977e+02  3.58910556e+01 -1.47169012e+02 -8.31259462e+01\n",
      " -6.77758365e+01  1.49724808e+02  3.70719965e+01 -2.73767908e+02\n",
      " -3.35433048e+01 -7.42459582e+01 -4.72957854e+01  2.60090003e+01\n",
      " -1.59808586e+02 -1.78440497e+02 -2.15152024e+02  8.93817265e+01\n",
      " -5.16101008e+01  1.70000738e+02 -9.39144540e+01  2.43395872e+02\n",
      " -6.81610392e+01 -1.12090700e+02  5.87319017e+01 -2.35737224e+02\n",
      "  4.53595932e+01  1.46756066e+02  1.73023578e+01  3.48649349e+01\n",
      " -1.00354562e+01 -8.52406985e+00 -2.39173034e+01 -4.34680327e+01\n",
      "  4.84911870e+01  8.79004971e+00  1.54212420e+01  2.36053055e+01\n",
      " -1.06311858e+02 -1.17606138e+01  2.42448930e+01  1.57856178e+00\n",
      " -2.94747002e+01 -1.15041756e+01  2.77751258e+01  3.00299774e+01\n",
      "  7.12935597e+01 -7.49133125e+00 -1.51027058e+02 -7.57099413e+01\n",
      " -1.28992318e+01 -8.12731357e+01 -3.85550274e+01 -7.21612322e+01\n",
      " -5.24613367e+01  9.97187261e+00  6.95663306e+01 -2.15863837e+01\n",
      " -1.26377160e+02  6.44950742e+01 -4.16154283e+01  7.33572220e+00\n",
      " -2.15487583e+01  2.59065366e+02  2.48691127e+01  6.97299302e+01\n",
      " -9.87286927e+01 -4.51676580e+01 -3.59520156e+01  1.90974433e+01\n",
      "  3.34375332e+01 -1.06599548e+01 -6.78388330e+01  2.79388224e+01\n",
      " -1.44996819e+01 -4.01139736e+01 -6.31419435e+01  6.99999998e+01\n",
      "  2.29918307e+01 -1.57451425e+01 -1.35118072e+02  4.93417071e+01\n",
      "  1.91951166e+00  8.77796772e+01 -1.00753281e+02 -2.66881757e+01\n",
      " -2.11891256e+02 -4.99427735e+00 -3.60498572e+01  8.37530284e+01\n",
      "  4.22358646e+01 -5.50940340e+00 -4.14146378e+01  4.52694672e+01\n",
      "  2.46239363e+01 -1.94151919e+01 -1.67670321e+01  7.74174783e-01\n",
      "  8.69442285e+00 -1.48405936e+01  1.46501997e+01 -8.41669778e+00\n",
      " -3.52567369e+01 -5.10872406e+01  2.91841682e+00  1.38294083e+01\n",
      "  3.28400717e+01  6.92921847e+00 -1.35468152e+03 -1.03994829e+02\n",
      "  5.47093692e+01  1.46540629e+00 -7.31426101e+00 -4.16574948e+01\n",
      "  6.65180551e+01 -2.01779454e+02  2.15564562e+01  1.42878603e+01\n",
      " -2.58326845e+01  2.17693746e+00 -4.19328180e+01  3.73114183e+01\n",
      " -1.66006756e+01 -6.02944931e+01 -2.39481194e+01  1.30552741e+01\n",
      "  5.87927299e+00  7.65841104e+00 -4.74093305e+01 -6.03935174e+02\n",
      " -6.14331114e+01 -3.90462292e+01  5.45345269e+01  5.27140866e+01\n",
      "  2.36221243e+01 -3.02701291e+01 -4.38131825e+01 -1.24155865e+01\n",
      "  2.29579243e+01 -2.81688983e+01  4.14425288e+01 -1.95504776e+01\n",
      "  1.32296895e+01  2.29082993e+01  2.98715048e+00  1.01799459e+01\n",
      " -1.51983042e+01 -6.15054904e+01  7.02387345e+01 -1.96819747e+02\n",
      "  1.47340543e+01 -4.04449940e+01 -5.02042948e+01  8.24211932e+01\n",
      " -4.93257273e+01 -3.57524699e+01  3.78744643e+01  7.51884681e+01\n",
      "  3.98051485e+01 -2.19747663e+01  9.46822773e+00 -3.90981413e+01\n",
      " -9.70742769e+00 -1.41870665e-02  3.47975318e+01 -1.98754708e+01\n",
      "  3.01660966e+01 -2.95016419e+01 -1.95577542e+02  4.64799173e+01\n",
      "  1.59552778e+01  1.51091841e+02 -5.04895861e+01 -4.74177001e+01\n",
      " -1.60312681e+01 -1.40788412e+01 -2.25106958e+01  1.78448941e+01\n",
      "  6.72112793e+00  1.06409344e+01  1.31113583e+01 -1.83650648e+01\n",
      " -8.94655932e+00  1.63760346e+01 -1.52506743e+01  2.86517618e+01\n",
      " -2.01064005e+01  2.27709295e+00 -8.46822112e+01 -1.20846627e+02\n",
      "  1.76504541e+01  3.37378007e+01 -4.69555697e+01 -4.92886903e+01\n",
      " -3.62516259e+00  4.52025448e+01 -1.64640738e+01  3.61359166e+01\n",
      "  6.64068106e+01  5.00388036e+01 -3.01535113e+01  1.79752542e+01\n",
      " -3.02523129e+01 -1.59105219e+02 -2.26177827e+01  4.40666243e+01\n",
      "  3.36097699e+01 -9.81641942e+00 -1.80458884e+00  1.03690843e+01\n",
      "  1.60350114e+01 -3.82255581e+01  2.25543192e+01 -4.27524651e+01\n",
      "  4.63117613e+01 -4.85247905e+01  1.53388520e+01 -2.63090093e+01\n",
      "  2.18068001e+01 -1.10725886e+02 -4.64293994e+01  8.07269962e+01\n",
      "  3.15637954e+01  5.98468608e+01  2.28030102e+01 -8.42039887e+00\n",
      " -9.91170477e+01 -2.20666134e+02 -2.06393259e+02 -5.16744886e+03\n",
      "  5.81626773e+01  4.13970100e+02 -6.65229283e+02  5.70559781e+02\n",
      "  5.66927919e+02 -9.84121214e+01 -2.93299859e+01  1.31164219e+02\n",
      "  1.45553330e+02  1.67913965e+02 -3.00345710e+02 -8.13378457e+02\n",
      "  1.08207729e+02 -5.11865548e+02 -3.85289392e+01 -1.22176610e+02\n",
      " -3.02581144e+02  2.87699732e+02  1.34520575e+02 -1.06695926e+02\n",
      " -3.21551858e+01 -1.53110636e+01  6.80788001e+01 -6.14765727e+01\n",
      " -4.66168784e+01 -5.76358285e+00  1.94201036e+02 -1.76325069e+01\n",
      "  2.48384368e+01  1.19909624e+02 -9.53938044e+01  1.31930682e+02\n",
      " -1.56950518e+02 -6.75845611e+01  4.89475817e+01 -6.38343646e+01\n",
      " -8.19302667e+01 -6.60016298e+01 -7.82304681e+01  1.15178493e+02\n",
      "  7.66324621e+00 -4.50391113e+00  1.90612742e+01  2.33187321e+02\n",
      " -2.55912032e+02 -1.12250328e+02 -1.66489597e+02 -1.34958711e+02\n",
      " -1.13146684e+02  4.03467916e+02  1.26485555e+02 -2.52316112e+01\n",
      "  4.43564351e+00 -1.32160547e+01  1.73801662e+02 -2.08890980e+02\n",
      " -2.88895318e+02 -2.82476698e+02 -1.24648227e+02  3.10661542e+02\n",
      "  1.09780023e+02  6.28982910e+01  4.69075361e+00  2.64070542e+02\n",
      "  2.48086236e+02 -4.87353708e+02 -4.02407354e+02 -9.59899539e+01\n",
      "  3.00979215e+02 -7.53484506e+00  7.63254648e+01  6.29025463e+01\n",
      "  1.69253975e+01  8.73076107e+02 -1.29670634e+03 -1.73503970e+02\n",
      " -7.56546491e+01  1.11873821e+02  2.03607304e+01  5.91667990e+01\n",
      "  2.23834300e+01  4.06568661e+02 -1.11570262e+02  4.75336419e+02\n",
      "  2.42926088e+01  5.52303391e+01  3.62226031e+01 -1.71784195e+01\n",
      "  4.17630760e+01 -1.12686874e+02  9.13217313e+01  5.22676532e+01\n",
      "  3.16455198e+01  5.99607497e+00  7.64899659e+02  5.88150194e+01\n",
      " -1.01137931e+01 -6.17244586e+01  3.39514556e+02  1.71577533e+02\n",
      " -1.22803692e+02 -7.33375559e+01 -6.91843640e+01  8.56894967e+01\n",
      "  4.39617264e+01 -1.07414064e+01 -1.26739395e+02 -6.01347475e+01\n",
      " -1.07554263e+01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing RMSE for Lasso: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100/100 [52:30<00:00, 31.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 10000.0\n",
      "Coefficients: [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0.  0.  0.  0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.  0.  0. -0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      "  0.  0.  0. -0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0.  0.  0. -0. -0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0. -0.\n",
      "  0.  0.  0. -0.  0.  0.  0.  0. -0. -0. -0. -0. -0. -0. -0.  0. -0.  0.\n",
      "  0.  0.  0.  0.  0. -0. -0. -0. -0. -0.  0.  0. -0. -0. -0. -0. -0.  0.\n",
      " -0. -0.  0. -0. -0.  0.  0.  0.  0. -0.  0.  0.  0. -0.  0. -0. -0. -0.\n",
      " -0.  0.  0. -0.  0.  0.  0.  0. -0.  0.  0.  0.  0.  0.  0. -0.  0.  0.\n",
      " -0.  0.  0. -0. -0. -0. -0. -0. -0. -0. -0.  0. -0. -0. -0.  0.  0. -0.\n",
      "  0.  0.  0.  0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  0. -0. -0. -0.  0.\n",
      "  0.  0.  0. -0.  0. -0.  0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0.  0.  0. -0. -0. -0. -0.  0. -0. -0.  0.  0.  0.  0. -0.  0. -0.  0.\n",
      "  0.  0. -0.  0. -0. -0. -0. -0. -0. -0. -0.  0. -0.  0. -0. -0.  0.  0.\n",
      " -0.  0.  0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0.  0. -0. -0. -0. -0. -0. -0.  0. -0. -0.\n",
      "  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0. -0.  0.  0.\n",
      "  0.  0. -0.  0.  0.  0.  0.  0. -0.  0.  0.  0. -0.  0. -0. -0. -0. -0.\n",
      " -0.  0.  0.  0.  0. -0.  0. -0.  0. -0.  0.  0.  0.  0.  0.  0.  0. -0.\n",
      "  0. -0. -0.  0.  0.  0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.  0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Increase the print options for NumPy arrays to display all elements\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Range for lambda\n",
    "lambdas = np.logspace(-4, 4, 100)\n",
    "\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "\n",
    "# Wrap the loop with tqdm for progress bar\n",
    "for lam in tqdm(lambdas, desc=\"Computing RMSE for Lasso\"):\n",
    "    lasso = Lasso(alpha=lam, max_iter=10000)  # Increased max_iter for convergence\n",
    "    lasso.fit(X_train_poly_scaled, y_train)\n",
    "    \n",
    "    # Training RMSE\n",
    "    y_pred_train = lasso.predict(X_train_poly_scaled)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    train_rmse.append(np.sqrt(mse_train))\n",
    "    \n",
    "    # Test RMSE\n",
    "    y_pred_test = lasso.predict(X_test_poly_scaled)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    test_rmse.append(np.sqrt(mse_test))\n",
    "    \n",
    "    # For observation: Print coefficients for a few lambdas\n",
    "    if lam in [1e-4, 1e-2, 1, 1e2, 1e4]:\n",
    "        print(f\"\\nLambda: {lam}\")\n",
    "        print(\"Coefficients:\", lasso.coef_)\n",
    "        \n",
    "# After the loop, you can reset the print options if needed\n",
    "np.set_printoptions(threshold=1000)  # Reset threshold to default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda: 0.0001 - Here, many coefficients are non-zero, and some are close to zero, which is expected with a small lambda in Lasso regularization.\n",
    "Lambda: 10000.0 - All coefficients are either zero or extremely close to zero, which is also expected since a high lambda value in Lasso pushes most coefficients towards zero due to the regularization penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.11.3.3**: Make a plot with the lambdas on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3469299a7668bf4275e2824d1d454144",
     "grade": false,
     "grade_id": "cell-5a2846b33750acbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambdas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot RMSE against lambdas\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlambdas\u001b[49m, train_rmse, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain RMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(lambdas, test_rmse, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest RMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxscale(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambdas' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot RMSE against lambdas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambdas, train_rmse, label='Train RMSE', color='blue')\n",
    "plt.plot(lambdas, test_rmse, label='Test RMSE', color='red')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.title('RMSE vs. Lambda')\n",
    "plt.show()\n",
    "\n",
    "# Bonus: Lambda that gives the lowest MSE-test score\n",
    "min_rmse_idx = test_rmse.index(min(test_rmse))\n",
    "best_lambda = lambdas[min_rmse_idx]\n",
    "print(f\"The lambda that gives the lowest MSE-test score is: {best_lambda}\")\n",
    "plt.savefig(\"rmse_vs_lambda.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forklarende tekst til den sidste exercise (forklaring af plottet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red Line (Test RMSE):\n",
    "\n",
    "Goes Down: Initially, as you increase the strength of regularization (i.e., increase lambda), the model becomes less overfit to the training data, and thus the test error might reduce.\n",
    "Goes Up: After a certain point, increasing lambda results in too much regularization. This causes the model to become underfit, meaning it's too simple to capture the underlying patterns in the data. As a result, both training and test error can start to increase.\n",
    "Becomes Stable: At very high lambda values, most of the coefficients in the Lasso regression approach zero. Therefore, further increases in lambda have minimal impact on the model's predictions, resulting in a relatively stable error.\n",
    "Blue Line (Train RMSE):\n",
    "\n",
    "Goes Up: As lambda increases, the model becomes more regularized and thus more biased. This results in an increase in the training error.\n",
    "Becomes Stable: Similar to the test RMSE, at high lambda values, the model becomes very simple, and further increases in lambda have minimal effect.\n",
    "When interpreting this:\n",
    "\n",
    "The lambda that minimizes the test RMSE is the optimal value as it strikes a balance between bias and variance.\n",
    "If the training RMSE is significantly lower than the test RMSE across most lambda values, it indicates that the model might be overfitting the training data.\n",
    "The stable pattern after a certain point simply indicates that the model has become too simple to learn any more from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lambda=0 then no penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # BASELINE MODEL WITH POLYNOMIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdata with Polynomial Features: Best alpha = 12.874614390685121\n",
      "Subdata with Polynomial Features: Training RMSE = 1652.2735628929356\n",
      "Subdata with Polynomial Features: Test RMSE = 1831.787143521524\n"
     ]
    }
   ],
   "source": [
    "# Subdata preparation\n",
    "X_sub = Data.drop(['Monthly rent', 'Longitude', 'Latitude', 'PostalCode_1000-1999', 'Floor_-1 to 0', \n",
    "                   'Distance to Transport Station (km)', 'Distance to Beach (km)', 'Distance to School (km)', \n",
    "                   'Distance to Restaurant (km)', 'Distance to Hospital (km)', 'Distance to Mall (km)'], axis=1)\n",
    "y_sub = Data['Monthly rent']\n",
    "\n",
    "# Generating polynomial features for subdata\n",
    "poly_sub = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_sub_poly = poly_sub.fit_transform(X_sub)\n",
    "\n",
    "X_train_sub_poly, X_test_sub_poly, y_train_sub, y_test_sub = train_test_split(X_sub_poly, y_sub, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the subdata\n",
    "scaler_sub_poly = StandardScaler().fit(X_train_sub_poly)\n",
    "X_train_sub_poly_scaled = scaler_sub_poly.transform(X_train_sub_poly)\n",
    "X_test_sub_poly_scaled = scaler_sub_poly.transform(X_test_sub_poly)\n",
    "\n",
    "# LASSO with cross-validation on subdata\n",
    "lasso_cv_sub_poly = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv_sub_poly.fit(X_train_sub_poly_scaled, y_train_sub)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_sub_poly = lasso_cv_sub_poly.predict(X_train_sub_poly_scaled)\n",
    "y_pred_test_sub_poly = lasso_cv_sub_poly.predict(X_test_sub_poly_scaled)\n",
    "\n",
    "# RMSE\n",
    "rmse_train_sub_poly = np.sqrt(mean_squared_error(y_train_sub, y_pred_train_sub_poly))\n",
    "rmse_test_sub_poly = np.sqrt(mean_squared_error(y_test_sub, y_pred_test_sub_poly))\n",
    "\n",
    "print(f\"Subdata with Polynomial Features: Best alpha = {lasso_cv_sub_poly.alpha_}\")\n",
    "print(f\"Subdata with Polynomial Features: Training RMSE = {rmse_train_sub_poly}\")\n",
    "print(f\"Subdata with Polynomial Features: Test RMSE = {rmse_test_sub_poly}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTENDED MODEL WITH NO POLYNOMIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data: Best alpha = 2.4949117452174607\n",
      "Full Data: Training RMSE = 1841.9265973833678\n",
      "Full Data: Test RMSE = 1952.0976268065021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "X_full = Data.drop(['Monthly rent'], axis=1)\n",
    "y_full = Data['Monthly rent']\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler_full = StandardScaler().fit(X_train_full)\n",
    "X_train_full_scaled = scaler_full.transform(X_train_full)\n",
    "X_test_full_scaled = scaler_full.transform(X_test_full)\n",
    "\n",
    "# LASSO with cross-validation\n",
    "lasso_cv_full = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv_full.fit(X_train_full_scaled, y_train_full)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_full = lasso_cv_full.predict(X_train_full_scaled)\n",
    "y_pred_test_full = lasso_cv_full.predict(X_test_full_scaled)\n",
    "\n",
    "# RMSE\n",
    "rmse_train_full = np.sqrt(mean_squared_error(y_train_full, y_pred_train_full))\n",
    "rmse_test_full = np.sqrt(mean_squared_error(y_test_full, y_pred_test_full))\n",
    "\n",
    "print(f\"Full Data: Best alpha = {lasso_cv_full.alpha_}\")\n",
    "print(f\"Full Data: Training RMSE = {rmse_train_full}\")\n",
    "print(f\"Full Data: Test RMSE = {rmse_test_full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL WITH NO POLYNOMIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdata: Best alpha = 2.2852440864309953\n",
      "Subdata: Training RMSE = 1964.4846954158882\n",
      "Subdata: Test RMSE = 2071.975318686298\n"
     ]
    }
   ],
   "source": [
    "# Subdata preparation\n",
    "X_sub = Data.drop(['Monthly rent', 'Longitude', 'Latitude', 'PostalCode_1000-1999', 'Floor_-1 to 0', \n",
    "                   'Distance to Transport Station (km)', 'Distance to Beach (km)', 'Distance to School (km)', \n",
    "                   'Distance to Restaurant (km)', 'Distance to Hospital (km)', 'Distance to Mall (km)'], axis=1)\n",
    "y_sub = Data['Monthly rent']\n",
    "\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_sub, y_sub, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the subdata\n",
    "scaler_sub = StandardScaler().fit(X_train_sub)\n",
    "X_train_sub_scaled = scaler_sub.transform(X_train_sub)\n",
    "X_test_sub_scaled = scaler_sub.transform(X_test_sub)\n",
    "\n",
    "# LASSO with cross-validation on subdata\n",
    "lasso_cv_sub = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv_sub.fit(X_train_sub_scaled, y_train_sub)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_sub = lasso_cv_sub.predict(X_train_sub_scaled)\n",
    "y_pred_test_sub = lasso_cv_sub.predict(X_test_sub_scaled)\n",
    "\n",
    "# RMSE\n",
    "rmse_train_sub = np.sqrt(mean_squared_error(y_train_sub, y_pred_train_sub))\n",
    "rmse_test_sub = np.sqrt(mean_squared_error(y_test_sub, y_pred_test_sub))\n",
    "\n",
    "print(f\"Subdata: Best alpha = {lasso_cv_sub.alpha_}\")\n",
    "print(f\"Subdata: Training RMSE = {rmse_train_sub}\")\n",
    "print(f\"Subdata: Test RMSE = {rmse_test_sub}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTENDED MODEL WITH POLYNOMIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data with Polynomial Features: Best alpha = 10.966283350940843\n",
      "Full Data with Polynomial Features: Training RMSE = 1365.0705414917456\n",
      "Full Data with Polynomial Features: Test RMSE = 1531.5694931700148\n",
      "Coefficients for Full Data with Polynomial Features:\n",
      "[-0.00000000e+00  0.00000000e+00 -2.93033525e+01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.36751978e+02  1.67967257e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -8.54149457e+01\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  4.81172786e+02 -0.00000000e+00  1.68732795e+03  0.00000000e+00\n",
      "  0.00000000e+00  4.68362494e+01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.53630565e+02  0.00000000e+00  0.00000000e+00\n",
      " -4.49525041e+00 -9.36009763e+01 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -8.31397041e+02 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -2.11120840e+02 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  1.49701493e+02  0.00000000e+00\n",
      " -0.00000000e+00  1.59247004e+02 -3.76947600e+01  0.00000000e+00\n",
      " -1.20871337e+01 -0.00000000e+00  0.00000000e+00  5.06098914e+01\n",
      "  0.00000000e+00 -2.08201403e+01 -7.74900610e+01 -5.44158811e+00\n",
      " -8.31195713e+01  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.46845275e+00 -0.00000000e+00 -3.49428707e+01\n",
      " -0.00000000e+00 -1.35403311e+01 -0.00000000e+00 -4.55011083e+01\n",
      " -3.72557899e+01 -1.53610641e+01 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.49669437e+02  3.78509867e+02  9.16930983e+01\n",
      " -6.46998261e+02  1.71839047e+02  9.72175944e+01  4.78798769e+02\n",
      "  1.46859738e+03  0.00000000e+00 -1.19323387e+01  0.00000000e+00\n",
      " -5.15769951e+00 -2.11097505e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  4.84889888e+02  0.00000000e+00\n",
      " -0.00000000e+00 -1.82523652e+02 -0.00000000e+00 -8.88085409e+01\n",
      " -0.00000000e+00 -1.62735421e+02 -1.58886831e+02  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.14981522e+02 -0.00000000e+00  0.00000000e+00  1.55939194e+02\n",
      " -1.82754883e+02  5.02606507e+01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  2.59367257e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.64156860e+02  2.42829801e+01\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -1.09092463e+02  2.07390504e+01  0.00000000e+00\n",
      "  4.95574267e+00 -0.00000000e+00 -6.77488122e+00 -2.42738146e+01\n",
      " -0.00000000e+00  1.11103183e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  5.41720980e+01  2.85773820e+01  0.00000000e+00  4.04556276e+01\n",
      "  0.00000000e+00 -1.33201427e+01  0.00000000e+00 -0.00000000e+00\n",
      "  5.55763037e+01  0.00000000e+00  1.59368550e+01 -7.21613275e+01\n",
      " -3.66152968e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.77144810e+01  4.06056810e+01 -0.00000000e+00 -3.19372007e+00\n",
      "  5.09141904e+00  1.72793995e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  2.70631813e+00  0.00000000e+00 -0.00000000e+00  4.03249187e+01\n",
      " -0.00000000e+00  0.00000000e+00  1.93425625e+01 -0.00000000e+00\n",
      " -0.00000000e+00 -1.90339593e+01 -1.67092991e+01  0.00000000e+00\n",
      " -6.58991394e+00  1.46881095e+01  0.00000000e+00  0.00000000e+00\n",
      " -1.19034656e+02  0.00000000e+00  2.25134767e+01 -0.00000000e+00\n",
      " -0.00000000e+00  2.80552200e+00 -2.52261851e+02  2.54975370e+01\n",
      "  2.61890575e+01  1.29521285e+01  0.00000000e+00 -2.97255403e+00\n",
      "  6.17896723e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -1.91005549e+01 -3.40343362e+01  0.00000000e+00\n",
      "  0.00000000e+00 -5.43177247e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -1.18683935e+01 -5.20287294e+01  5.96856788e+01\n",
      "  1.41325413e+00  0.00000000e+00 -3.08900184e+01 -2.35421266e+01\n",
      "  0.00000000e+00  0.00000000e+00 -2.74368060e+01  0.00000000e+00\n",
      "  0.00000000e+00  2.83786578e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.81042762e+01 -7.43165332e+00 -9.75491597e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -2.11983794e+01 -1.89640734e+01\n",
      "  8.42531108e+00 -1.01864473e+02 -2.11630731e+01  4.00301551e+01\n",
      "  5.85584876e+01  3.22697563e+01  0.00000000e+00  0.00000000e+00\n",
      " -2.12800079e+01 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -1.55985140e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  8.94258806e+00 -8.46725826e-01  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.83465924e+01 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  8.28049734e+00  0.00000000e+00\n",
      " -1.71058937e+02  0.00000000e+00  3.22194239e+01 -1.03108096e+01\n",
      " -4.93911412e+01  0.00000000e+00  9.23264112e+01  7.42612924e+01\n",
      " -1.46752983e+01  7.37967388e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.85756734e+00  7.83422991e+01 -3.33448910e+01\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  2.65502434e+01  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -3.81616484e+00  0.00000000e+00 -7.20540818e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  5.16504147e+01 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -5.22871022e+01 -0.00000000e+00\n",
      " -3.75862088e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.26211885e+01\n",
      "  8.40660478e+01 -0.00000000e+00 -1.72397244e+01  0.00000000e+00\n",
      " -1.92383309e+02  0.00000000e+00  8.95724110e+01  0.00000000e+00\n",
      "  7.91337322e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.85068452e+01  6.01965409e+01 -0.00000000e+00\n",
      " -0.00000000e+00  2.42774394e+01 -4.34140640e+02 -0.00000000e+00\n",
      " -9.26223076e+01 -1.45722410e+02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -2.74419439e+01  1.23802767e+01  0.00000000e+00  1.75105395e+01\n",
      " -1.43347687e+01  4.95648223e+01  6.10645836e+01 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  8.01373783e+00  0.00000000e+00\n",
      " -2.15768798e+01  0.00000000e+00 -0.00000000e+00 -6.59194399e+01\n",
      "  1.96245898e+01  0.00000000e+00  0.00000000e+00 -3.31332512e+01\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -2.86870690e+01\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -2.14711749e+01 -2.43976795e+01 -2.93383754e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -1.58542644e+01  7.23425817e+00  0.00000000e+00\n",
      "  2.17919637e+01  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  6.28173233e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.51400096e+01 -1.30308821e+01 -0.00000000e+00\n",
      "  1.69911517e+01 -3.69807976e+01 -2.74268399e+01  1.24864181e+01\n",
      "  4.80917399e+01  0.00000000e+00  5.11411426e+02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.92254500e+01\n",
      " -1.31976477e+01 -6.42964671e+02  0.00000000e+00 -4.21966395e+01\n",
      "  4.88456555e+00 -0.00000000e+00 -0.00000000e+00 -6.01622655e+00\n",
      " -0.00000000e+00  0.00000000e+00  1.06367624e+01 -0.00000000e+00\n",
      "  0.00000000e+00 -2.75663898e+01 -9.22319733e-01 -0.00000000e+00\n",
      " -2.00463586e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.97542489e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -3.44931131e+00 -0.00000000e+00 -5.13199513e+00\n",
      " -0.00000000e+00  0.00000000e+00 -5.29782717e+01 -9.62768480e+01\n",
      "  4.13442132e+01 -6.41264752e+01  8.07644616e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.76324915e+01  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.99233328e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.44485245e+02  0.00000000e+00  0.00000000e+00\n",
      "  2.13491771e+01  1.60678837e+02  0.00000000e+00 -1.89202840e+01\n",
      " -3.58929371e+01  6.21109063e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Generating polynomial features\n",
    "poly_full = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_full_poly = poly_full.fit_transform(X_full)\n",
    "\n",
    "X_train_full_poly, X_test_full_poly, y_train_full, y_test_full = train_test_split(X_full_poly, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler_full_poly = StandardScaler().fit(X_train_full_poly)\n",
    "X_train_full_poly_scaled = scaler_full_poly.transform(X_train_full_poly)\n",
    "X_test_full_poly_scaled = scaler_full_poly.transform(X_test_full_poly)\n",
    "\n",
    "# LASSO with cross-validation\n",
    "lasso_cv_full_poly = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv_full_poly.fit(X_train_full_poly_scaled, y_train_full)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_full_poly = lasso_cv_full_poly.predict(X_train_full_poly_scaled)\n",
    "y_pred_test_full_poly = lasso_cv_full_poly.predict(X_test_full_poly_scaled)\n",
    "\n",
    "# RMSE\n",
    "rmse_train_full_poly = np.sqrt(mean_squared_error(y_train_full, y_pred_train_full_poly))\n",
    "rmse_test_full_poly = np.sqrt(mean_squared_error(y_test_full, y_pred_test_full_poly))\n",
    "\n",
    "print(f\"Full Data with Polynomial Features: Best alpha = {lasso_cv_full_poly.alpha_}\")\n",
    "print(f\"Full Data with Polynomial Features: Training RMSE = {rmse_train_full_poly}\")\n",
    "print(f\"Full Data with Polynomial Features: Test RMSE = {rmse_test_full_poly}\")\n",
    "\n",
    "print(\"Coefficients for Full Data with Polynomial Features:\")\n",
    "print(lasso_cv_full_poly.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
